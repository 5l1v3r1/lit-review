# Transformer on a Diet
*Chenguang Wang, Zihao Ye, Aston Zhang, Zheng Zhang, Alexander J. Smola*
Amazon
[https://arxiv.org/abs/2002.06170]

## Overview:

In this paper, the researchers attempt to substantially reduce the size of the tranformer architecture for the language modeling task. They argue that many current models are unwieldy for production tasks (EX: GPT-2 with 1.6B parameters).
