# Transformer on a Diet
*Chenguang Wang, Zihao Ye, Aston Zhang, Zheng Zhang, Alexander J. Smola*
Amazon
[https://arxiv.org/abs/2002.06170]
(hat tip to Cengiz Tirkaz for the find)

## Overview:

In this paper, the researchers attempt to substantially reduce the size of the tranformer architecture for the language modeling task. They argue that many current models are unwieldy for production tasks (EX: GPT-2 with 1.6B parameters).

![](Figures/transformer-diet-1.png)

## Architectures
The researchers explore four different architectures, as shown above and described below.
