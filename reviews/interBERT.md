
## InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining

Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, Hongxia Yang

Alibaba, MOE Key Lab of Computational Linguistics Peking U

2020 -- KDD

### Overview


### Architecture
The architecture is shown below. It consists of separate image and text embedding layers, followed by a joint "single-stream interaction module," followed by separate "independence modules."

![fig 1](Figures/interBERT-1.png)

### Datasets

The following datasets were used for pre-training and for downstream tasks.

![fig_2](Figures/interBERT-2.png)
